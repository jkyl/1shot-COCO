{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from textgen import *\n",
    "\n",
    "m = TextGen(\n",
    "    img_size=299, \n",
    "    code_dim=512, \n",
    "    rnn_type='LSTM', \n",
    "    cnn_type='InceptionV3',\n",
    "    pooling='avg',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model, val_record, ckpt, batch_size=32, capacity=8, n_threads=6, epoch_size=10000, \n",
    "             captions_json='/home/paperspace/data/ms_coco/captions_val2014.json',\n",
    "             inds_to_words_json='/home/paperspace/data/ms_coco/SOS_preproc_vocab-9413_threshold-5_length-20/inds_to_words.json'):\n",
    "    ''''''\n",
    "    with tf.variable_scope('BatchReader'):\n",
    "        print('creating batch reader')\n",
    "        coord =  tf.train.Coordinator()\n",
    "\n",
    "        # read and preprocess training records\n",
    "        x, y, _ = model.read_tfrecord(\n",
    "            val_record, \n",
    "            batch_size=batch_size, \n",
    "            capacity=capacity, \n",
    "            n_threads=n_threads,\n",
    "            n_epochs=None)\n",
    "        x = model.preproc_img(x)\n",
    "        y, _ = model.preproc_caption(y, random=False)\n",
    "\n",
    "    with tf.variable_scope('Strings'):\n",
    "        print('getting strings')\n",
    "\n",
    "        y_hat = model.sample_recursively(x, continuous=False)\n",
    "        table = model.create_table(inds_to_words_json)\n",
    "        y_hat_strings = model.postproc_caption(y_hat, table)[0]\n",
    "\n",
    "    with tf.Session(graph=model.graph) as sess:\n",
    "        print('starting threads')\n",
    "        tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        print('initializing variables')\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        print('loading weights from '+ckpt)\n",
    "        model.load_weights(ckpt)\n",
    "        \n",
    "        print('loading IDs from '+captions_json)\n",
    "        ids = sorted([e['id'] for e in \\\n",
    "                json.loads(open(captions_json)\\\n",
    "                   .read())['images']])\n",
    "\n",
    "        n = 0\n",
    "        results = []\n",
    "        try:\n",
    "            for _ in tqdm.trange(epoch_size):\n",
    "                if not coord.should_stop():\n",
    "                    s = sess.run(y_hat_strings, {backend.learning_phase(): False})\n",
    "                    id_ = ids[n]\n",
    "                    for i, c in enumerate(s):\n",
    "                        d = {\"image_id\": id_[i], \"caption\": c}\n",
    "                    results.append(d)\n",
    "                    n += 1\n",
    "                else:\n",
    "                    raise IOError, 'queues closed'\n",
    "            return results\n",
    "        # exit behaviour: request thread stop, then wait for \n",
    "        # them to recieve message before exiting session context\n",
    "        except:\n",
    "            coord.request_stop()\n",
    "            time.sleep(0.2)\n",
    "            raise"
    "    with tf.device('/cpu:0'):\n",
    "        with tf.variable_scope('BatchReader'):\n",
    "            print('creating batch reader')\n",
    "\n",
    "            # coordinator for tf queues\n",
    "            coord =  tf.train.Coordinator()\n",
    "\n",
    "            # read and preprocess training records\n",
    "            x, y, cls = model.read_tfrecord(\n",
    "                val_record, \n",
    "                batch_size=batch_size, \n",
    "                capacity=capacity, \n",
    "                n_threads=n_threads,\n",
    "                n_epochs=None)\n",
    "            x = model.preproc_img(x)\n",
    "            y, _ = model.preproc_caption(y, random=False)\n",
    "\n",
    "        #with tf.variable_scope('StagingArea'):\n",
    "        #    print('creating staging area')\n",
    "\n",
    "        #    # create training queue on GPU\n",
    "        #    get, size, put = model.stage_data(\n",
    "        #        [x, y, cls], capacity=capacity)\n",
    "        #    x, y, cls = get\n",
    "\n",
    "        with tf.variable_scope('Metrics'):\n",
    "            print('getting metrics')\n",
    "\n",
    "            y_hat = model.sample_recursively(x, continuous=False)\n",
    "            table = model.create_table(inds_to_words_json)\n",
    "\n",
    "            y_inds, y_hat_inds = [tf.argmax(c, axis=-1) for c in (y, y_hat)]\n",
    "            y_strings, y_hat_strings = [tf.gather(table, c) for c in (y_inds, y_hat_inds)]\n",
    "\n",
    "            def bleu(data, pred):\n",
    "                bs = data.shape[0]\n",
    "                score = 0\n",
    "                for i in range(bs):\n",
    "                    y, y_hat = [[w for w in words if not w in ('SOS', 'EOS')] \\\n",
    "                                for words in data[i], pred[i]]\n",
    "                    score += sentence_bleu([y], y_hat, weights=[1], \n",
    "                                           smoothing_function=SmoothingFunction().method3)\n",
    "                score /= bs\n",
    "                return np.array(score, dtype='float32')\n",
    "\n",
    "            bleu_score = tf.py_func(bleu, [y_strings, y_hat_strings], tf.float32)\n",
    "\n",
    "        with tf.Session(graph=model.graph) as sess:\n",
    "            print('starting threads')\n",
    "\n",
    "            tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "            #stop, threads = model.start_threads(\n",
    "            #    sess, put, n_stage_threads=n_threads)\n",
    "\n",
    "            print('initializing variables')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            print('loading weights from '+ckpt)\n",
    "            model.load_weights(ckpt)\n",
    "\n",
    "            #print('finalizing graph')\n",
    "            #model.graph.finalize()\n",
    "            \n",
    "            classes = set([67, 14, 75, 53, 64, 86, 56, 50, 76, 37, \n",
    "                           89, 57, 70, 41, 33, 43, 40, 60, 88, 72,  \n",
    "                            4, 54,  2, 79, 80,  3, 47, 20, 65, 82, \n",
    "                           32, 19, 17, 46, 27, 18,  9, 74, 49, 39])\n",
    "            n = 0.\n",
    "            in_ = 0.\n",
    "            out = 0.\n",
    "            try:\n",
    "                for _ in tqdm.trange(epoch_size):\n",
    "                    if not coord.should_stop():\n",
    "                        return sess.run([cls, x], {backend.learning_phase(): False})\n",
    "                        n += 1\n",
    "                        if len(set(c) - classes) == 0:\n",
    "                            in_ += 1.\n",
    "                        else:\n",
    "                            out += 1.\n",
    "                    else:\n",
    "                        raise IOError, 'queues closed'\n",
    "                print(in_/out)\n",
    "            # exit behaviour: request thread stop, then wait for \n",
    "            # them to recieve message before exiting session context\n",
    "            except:\n",
    "                coord.request_stop()\n",
    "                time.sleep(0.2)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = validate(m, \n",
    " '/home/paperspace/data/ms_coco/SOS_preproc_vocab-9413_threshold-5_length-20/val_1shot.tfrecord',\n",
    " '/home/paperspace/training/1shot/lr4e-4_decay100k_bs16_reg1e-4_clip50_baseclasses/ckpt_update-0000890000.h5', \n",
    "batch_size=32)"
    "c, x = validate(m, \n",
    " '/home/paperspace/data/ms_coco/SOS_preproc_vocab-9413_threshold-5_length-20/train_2shot.tfrecord',\n",
    " '/home/paperspace/training/20shot/lr4e-4_decay25k_bs64_reg1e-4_clip1.0/ckpt_update-0000130000.h5', \n",
    " batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and preprocess training records\n",
    "coord = tf.train.Coordinator()\n",
    "x, y, cls = m.read_tfrecord(\n",
    "    '/home/paperspace/data/ms_coco/SOS_preproc_vocab-9413_threshold-5_length-20/val_1shot.tfrecord', \n",
    "    batch_size=1, \n",
    "    capacity=1, \n",
    "    n_threads=1,\n",
    "    n_epochs=None)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    for i in range(5):\n",
    "        print(sess.run(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([e['id'] for e in \\\n",
    "        json.loads(open('/home/paperspace/data/ms_coco/captions_val2014.json')\\\n",
    "                   .read())['images']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
