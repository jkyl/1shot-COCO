{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert captions to lists of token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import json\n",
    "import time\n",
    "%pylab inline\n",
    "\n",
    "THRESHOLD = 5\n",
    "\n",
    "j_train = json.loads(open('/home/paperspace/data/ms_coco/captions_train2014.json').read())\n",
    "j_val = json.loads(open('/home/paperspace/data/ms_coco/captions_val2014.json').read())\n",
    "train = {d['caption']: d['image_id'] for d in j_train['annotations']}\n",
    "val = {d['caption']: d['image_id'] for d in j_val['annotations']}\n",
    "\n",
    "print('filtering non-alphabetic characters'); time.sleep(.2)\n",
    "for d in (train, val):\n",
    "    for k in tqdm.tqdm(d.keys()):\n",
    "        f = re.sub('[^a-z \\h]', ' ', k.lower())\n",
    "        d[f] = d.pop(k)\n",
    "\n",
    "print('splitting on spaces and hashing'); time.sleep(.2)\n",
    "total = train.keys() + val.keys()\n",
    "instances = {}\n",
    "for caption in tqdm.tqdm(total):\n",
    "    for w in caption.split(' '):\n",
    "        if w != '':\n",
    "            try:\n",
    "                instances[w] += 1\n",
    "            except:\n",
    "                instances[w] = 1\n",
    "\n",
    "print('filtering stopwords')\n",
    "words = sorted(instances.keys())\n",
    "words_set = set(words)\n",
    "#nltk_stopwords = set(w for w in open('stopwords.txt').read().split('\\n') if w != '')\n",
    "go_words = set([w for (w, c) in instances.items() if c > THRESHOLD])\n",
    "stop_words = words_set - go_words\n",
    "\n",
    "words_to_inds = dict(zip(sorted(list(go_words)), range(len(go_words))))\n",
    "inds_to_words = dict(zip(range(len(go_words)), sorted(list(go_words))))\n",
    "\n",
    "words_to_inds['UNKNOWN'] = len(go_words)\n",
    "inds_to_words[len(go_words)] = 'UNKNOWN'\n",
    "\n",
    "\n",
    "print('converting captions to indices'); time.sleep(.2)\n",
    "for d in (train, val):\n",
    "    for k in tqdm.tqdm(d.keys()):\n",
    "        v = d.pop(k)\n",
    "        split = k.split(' ')\n",
    "        split_caption = []\n",
    "        for w in split:\n",
    "            if w != '':\n",
    "                if w in stop_words:\n",
    "                    w = 'UNKNOWN'\n",
    "                split_caption.append(w)\n",
    "                \n",
    "        indices = [words_to_inds[w] for w in split_caption]\n",
    "        try:\n",
    "            d[v].append(indices)\n",
    "        except:\n",
    "            d[v] = []\n",
    "            d[v].append(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find an appropriate maximum caption length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_length = 20\n",
    "\n",
    "maxlen = 0\n",
    "total = []\n",
    "for d in (train, val):\n",
    "    for k, v in d.items():\n",
    "        for i, c in enumerate(v):\n",
    "            size = len(c)\n",
    "            total.append(size)\n",
    "            if size > maxlen:\n",
    "                maxlen = size\n",
    "                max_id = k\n",
    "                max_i = i\n",
    "                max_d = d\n",
    "                \n",
    "total = array(total)\n",
    "total.sort()\n",
    "print('max length: '+str(maxlen)) \n",
    "print('mean length: '+str(total.mean()))\n",
    "print('median length: '+str(median(total)))\n",
    "print('standard deviation: '+str(total.std()))\n",
    "print('percent of total captions with greater than 20 words: {:.2f}%'\\\n",
    "      .format(100 * (total > target_length).sum() / float(total.size)))\n",
    "plot(total, label='word length')\n",
    "plot(arange(total.size), target_length*ones(total.size), label='length='+str(target_length))\n",
    "legend(loc='best')\n",
    "grid(which='both')\n",
    "print('\\n' + ' '.join([inds_to_words[i] for i in max_d[max_id][max_i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate captions and append EOF token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(words_to_inds)\n",
    "words_to_inds['EOF'] = n\n",
    "inds_to_words[n] = 'EOF'\n",
    "\n",
    "for d in (train, val):\n",
    "    for k, v in d.items():\n",
    "        n_captions = len(v)\n",
    "        if n_captions < 7:\n",
    "            d[k] += [[]]*(7-n_captions)\n",
    "        for i, c in enumerate(d[k]):\n",
    "            v[i] = c[:target_length-1]\n",
    "            v[i] += [n]\n",
    "            if len(v[i]) < target_length:\n",
    "                v[i] += [-1]*(target_length-len(v[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_dir = '/home/paperspace/data/ms_coco/preproc/preproc_vocab-{}_threshold-{}_length-{}'\\\n",
    "             .format(len(words_to_inds), THRESHOLD, target_length)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "with open(os.path.join(output_dir, 'train_captions.json'), 'w') as outfile:\n",
    "    json.dump(train, outfile)\n",
    "with open(os.path.join(output_dir, 'val_captions.json'), 'w') as outfile:\n",
    "    json.dump(val, outfile)\n",
    "with open(os.path.join(output_dir, 'words_to_inds.json'), 'w') as outfile:\n",
    "    json.dump(words_to_inds, outfile)\n",
    "with open(os.path.join(output_dir, 'inds_to_words.json'), 'w') as outfile:\n",
    "    json.dump(inds_to_words, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from serialize_data import main\n",
    "main(\n",
    "    '/home/paperspace/data/ms_coco/train2014/',\n",
    "    '/home/paperspace/data/ms_coco/preproc/preproc_vocab-9412_threshold-5_length-20/train_captions.json',\n",
    "    '/home/paperspace/data/ms_coco/instances_train2014.json',\n",
    "    '/home/paperspace/data/ms_coco/preproc/preproc_vocab-9412_threshold-5_length-20/train_crop_299_fullshot.tfrecord'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
